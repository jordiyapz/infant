{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "import random\n",
    "from IPython.display import display\n",
    "import asyncio\n",
    "from promise import Promise\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.io import decode_png\n",
    "import torchsummary\n",
    "\n",
    "from infant import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefer_gpu():\n",
    "    return 'cuda:0' if T.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://localhost:3000'\n",
    "device = prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetModel(nn.Module):\n",
    "    def __init__(self, output_dims=1024):\n",
    "        super(ConvNetModel, self).__init__()\n",
    "        self.device = prefer_gpu()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=2),\n",
    "            # nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 5, kernel_size=5, stride=2),\n",
    "            # nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(16920, output_dims),\n",
    "            nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "    def forward(self, x:T.Tensor):\n",
    "        x = self.conv_layer(x.to(prefer_gpu()))\n",
    "        x = self.bottleneck(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetModel()\n",
    "# torchsummary.summary(model, T.zeros(1, 3, 400, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuNetModel(nn.Module):\n",
    "    def __init__(self, lr=1e-5, input_dims=1024, h1_dims=64, h2_dims=64, output_dims=3, device=None):\n",
    "        super(NeuNetModel, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dims, h1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dims, h2_dims),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(h2_dims, output_dims)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.device = device if device else prefer_gpu()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x:T.Tensor):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.device = prefer_gpu()\n",
    "        self.lr = lr\n",
    "        self.mu = nn.Sequential(nn.Linear(input_dims, n_actions), \n",
    "                                nn.Tanh())\n",
    "        self.var = nn.Sequential(nn.Linear(input_dims, n_actions),\n",
    "                                nn.ReLU())\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x:T.Tensor) -> Tuple[T.Tensor, T.Tensor]:\n",
    "        x = x.to(self.device)\n",
    "        mu:T.Tensor = self.mu(x)\n",
    "        var:T.Tensor = self.var(x)\n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha, beta, gamma=.99, epsilon=.2, max_replay=1_000, n_actions=3):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self._current_v = None\n",
    "        self._timestep_ct = 0\n",
    "        self._replay_memory = deque(maxlen=max_replay)\n",
    "        self.n_actions = n_actions\n",
    "        self.device = prefer_gpu()\n",
    "        self.log_probs = None\n",
    "\n",
    "        self.knowledges = [ConvNetModel()]\n",
    "        self.input_layer = ConvNetModel().to(self.device)\n",
    "        self.global_knowledge = NeuNetModel(alpha, output_dims=2, device=self.device)\n",
    "        self.critic = NeuNetModel(beta, output_dims=1, device=self.device)\n",
    "        # self.critic = nn.Sequential(nn.Linear(128, 1)).to(device)\n",
    "        # self.global_knowledge = nn.Sequential(nn.Linear(128, self.n_actions)).to(device)\n",
    "        self.actor = self.global_knowledge\n",
    "    \n",
    "    def save(self, pathname:str):\n",
    "        T.save({\n",
    "            'input_layer': self.input_layer.state_dict(),\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'actor_op': self.actor.optimizer.state_dict(),\n",
    "            'critic_op': self.critic.optimizer.state_dict(),\n",
    "        }, pathname)\n",
    "\n",
    "    def load(self, pathname:str):\n",
    "        checkpoint:dict = T.load(pathname)\n",
    "        self.input_layer.load_state_dict(checkpoint['input_layer']),\n",
    "        self.actor.load_state_dict(checkpoint['actor']),\n",
    "        self.critic.load_state_dict(checkpoint['critic']),\n",
    "        self.actor.optimizer.load_state_dict(checkpoint['actor_op']),\n",
    "        self.critic.optimizer.load_state_dict(checkpoint['critic_op']),\n",
    "        \n",
    "    def choose_action(self, observation:T.Tensor) -> T.Tensor:\n",
    "        # if T.rand(1)[0] < self.epsilon:\n",
    "        #     action = T.rand(3) * 2 - 1\n",
    "        # else:\n",
    "            # knowledge:nn.Module = random.choice(self.knowledges)\n",
    "            # x = knowledge(observation)\n",
    "        observation = observation.to(device=self.device)\n",
    "        x = self.input_layer(observation)\n",
    "\n",
    "        mu, sigma = T.squeeze(self.actor(x))\n",
    "        sigma = T.exp(sigma)\n",
    "        action_probs = T.distributions.Normal(mu, sigma)\n",
    "        probs = action_probs.sample(sample_shape=T.Size((self.n_actions,)))\n",
    "        self.log_probs = action_probs.log_prob(probs).to(self.actor.device)\n",
    "        action = T.tanh(probs) \n",
    "\n",
    "        # self._current_v = self.critic(observation)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def train(self, state:T.Tensor, reward:float, next_state:T.Tensor, done:bool):\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            x = self.input_layer(state.to(self.device))\n",
    "            x_ = self.input_layer(next_state.to(self.device))\n",
    "            critic_value = self.critic(x)\n",
    "            critic_value_ = self.critic(x_)\n",
    "        except Exception as e:\n",
    "            print('Critic: ', e)\n",
    "            print(state.shape, next_state.shape)\n",
    "            # print(x, x_)\n",
    "            return\n",
    "            \n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.device)\n",
    "        delta = reward + self.gamma * critic_value_ * (1-int(done)) - critic_value\n",
    "        \n",
    "        actor_loss:T.Tensor = -self.log_probs * delta\n",
    "        critic_loss:T.Tensor = delta**2\n",
    "\n",
    "        # print(actor_loss.shape, critic_loss.shape)\n",
    "        # (actor_loss + critic_loss).backward()\n",
    "        grand_loss:T.Tensor = actor_loss + critic_loss\n",
    "        grand_loss.backward(T.ones_like(grand_loss))  \n",
    "\n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "        # v_next = self.critic(next_state)\n",
    "        # advantage = reward + v_next - self._current_v\n",
    "        # loss = T.log(action, state) * advantage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_responder(agent: Agent, action_multiplier=20):\n",
    "    def responder(observation: T.Tensor, reward: T.Tensor, env: Environment):\n",
    "        \"\"\"Get observation and returns action\"\"\"\n",
    "        if reward > 0:\n",
    "            print('Reward: ', reward)\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            image = observation / 255\n",
    "            state = T.unsqueeze(image, dim=0)\n",
    "\n",
    "            # Train the agent\n",
    "            if env.prev_state != None:\n",
    "                try:\n",
    "                    agent.train(env.prev_state, reward, state, env._is_done)\n",
    "                except Exception as e:\n",
    "                    print('Training Error')\n",
    "                    print(e)\n",
    "            \n",
    "            # Predict correct action\n",
    "            action = agent.choose_action(state)\n",
    "            action = T.squeeze(action) * action_multiplier\n",
    "\n",
    "            env.prev_state = state\n",
    "            \n",
    "            return action\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            env.stop()\n",
    "            return T.ones((3,))\n",
    "\n",
    "    return responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model-can-9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(alpha=5e-6, beta=1e-5, epsilon=.8)\n",
    "agent.load(f'{model_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_id = \"726568c7\" or None\n",
    "env = Environment(sim_id) \n",
    "if sim_id is None:\n",
    "    env.create()\n",
    "    print(env.sim_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to /sim-726568c7\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.on_state(create_responder(agent))\n",
    "env.connect(disconnect_on_done=True)\n",
    "env.init(max_episodes=100)\n",
    "env.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.disconnect()\n",
    "# env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(f'{model_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using step fn from environment API.\n",
    "\n",
    "# loop = asyncio.get_running_loop()\n",
    "# env.set_running_loop(loop)\n",
    "\n",
    "# env.connect(close_on_stop=True)\n",
    "# fut = env.step(T.tensor([10,0,0], dtype=T.float))\n",
    "# data_uri, reward = await fut\n",
    "# data_uri[30:100], reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Connecting...\n",
      "Connected to /sim-4e7732c7\n",
      "Connected to /sim-688daa83\n",
      "Starting...\n",
      "Working...\n",
      "Cleaning...\n"
     ]
    }
   ],
   "source": [
    "# envs = [Environment() for _ in range(2)]\n",
    "\n",
    "# print('Initializing...')\n",
    "# for env in envs:\n",
    "#     env.create()\n",
    "#     agent = Agent().to(device)\n",
    "#     env.on_state(create_responder(agent))\n",
    "\n",
    "# print('Connecting...')\n",
    "# for env in envs:\n",
    "#     env.connect()\n",
    "\n",
    "# print('Starting...')\n",
    "# for env in envs:\n",
    "#     env.init(max_episodes=10)\n",
    "\n",
    "# print('Working...')\n",
    "# for env in envs:\n",
    "#     env.wait()\n",
    "\n",
    "# print('Cleaning...')\n",
    "# for env in envs:\n",
    "#     env.destroy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d24d54cb0694e031751d62983236527ac81d1b282769bdbede55422936476cda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
